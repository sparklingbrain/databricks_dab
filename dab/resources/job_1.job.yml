# The main job for dab.
resources:
  jobs:
    job_1:
      name: job_1

      trigger:
        # Run this job every day, exactly one day from the last run; see https://docs.databricks.com/api/workspace/jobs/create#trigger
        periodic:
          interval: 1
          unit: DAYS

      #email_notifications:
      #  on_failure:
      #    - your_email@example.com

      tasks: 
        - task_key: job1_t1
          # job_cluster_key: job_cluster_1
          notebook_task:
            # notebook_path: ../src/notebook.ipynb
            notebook_path: ${var.dab_notebook}
            base_parameters:
              t1_param: "t1_param_value"

        - task_key: job_2_trigger
          depends_on: 
            - task_key: job1_t1
          # job_cluster_key: job_cluster_1
          run_job_task: 
            job_id: ${resources.jobs.job_2.id}
            job_parameters:
              param_from_job_1: "{{tasks.job1_t1.values.from_t1}}"
              # This is the job_id of the job that will be triggered.
              # It can be a different job defined in the same bundle or an existing job in the workspace.
              # See https://docs.databricks.com/api/workspace/jobs/create#run_job_task
              # Note: The job_id should be referenced as ${resources.jobs.job_2.id} if it is defined in the same bundle.


      
        # - task_key: notebook_task
        #   # job_cluster_key: job_cluster_1
        #   notebook_task:
        #     # notebook_path: ../src/notebook.ipynb
        #     notebook_path: ${var.dab_notebook}
        #     base_parameters:
        #       notebook_param: "my_param_value"
              # limit_param: 1
            
        # - task_key: notebook_task_2
        #   # job_cluster_key: job_cluster_2
        #   notebook_task:
        #     # notebook_path: ../src/notebook.ipynb
        #     notebook_path: ${var.dab_notebook}

        # - task_key: notebook_task_3
        #   existing_dab_cluster: {var.existing_dab_cluster}            
        #   notebook_task:
        #     notebook_path: ${var.dab_notebook}

      queue:
        enabled: true
      performance_target: PERFORMANCE_OPTIMIZED
      # spark_conf:
      #   spark.speculation: true
      #   partitionDate: ${var.partitionDate}

      # job_clusters:
      #   - job_cluster_key: job_cluster_1
      #     new_cluster: ${var.dab_cluster_task_1}

      #   - job_cluster_key: job_cluster_2
      #     new_cluster: ${var.dab_cluster_task_2}

      #   - job_cluster_key: job_cluster_default
      #     new_cluster:
      #       spark_version: 15.4.x-scala2.12
      #       node_type_id: i3.xlarge
      #       data_security_mode: SINGLE_USER
      #       autoscale:
      #         min_workers: 1
      #         max_workers: 4
